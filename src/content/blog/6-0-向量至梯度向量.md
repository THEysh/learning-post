---
author: 杨盛晖
pubDatetime: 2024-11-06T14:59:00+08:00
modDatetime: 
title: 从单位向量到梯度
slug: cFsIcArFKJnfskjgOsho
featured: true
draft: false
tags:
  - 人工智能数学基础
description:
  介绍了向量的基本概念及其运算，包括向量的定义、表示方法、大小（模）、单位向量、方向、向量加法、标量乘法、内积和外积。向量运算遵循加法的交换律、结合律以及标量乘法的结合律和分配律。文章进一步讨论了向量空间和线性组合的概念。接着，重点介绍了梯度向量，它是多变量函数的偏导数向量，表示函数在某一点的最陡上升方向。最后，文章通过梯度下降法的应用实例，展示了如何利用梯度向量解决最优化问题，通过不断更新位置来最小化目标函数。
---

# 向量与梯度向量的介绍

## 1. 向量

### 1.1 定义
向量是一个有大小和方向的量。它通常表示为一个有序的数值列表。

### 1.2 表示
在数学中，向量可以用不同的方式表示：
- **列向量**： 
  \[
  \mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
  \]
- **行向量**：
  \[
  \mathbf{v} = \begin{pmatrix} v_1 & v_2 & v_3 \end{pmatrix}
  \]

### 1.3 向量表示

#### 1.3.1 二维向量的表示
在二维空间中，向量通常表示为一个有序对，记作：
\[
\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}
\]
例如，向量 \(\mathbf{a}\) 可以具体表示为：
\[
\mathbf{a} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}
\]
这里，\(a_1 = 3\) 是向量在 x 轴上的分量，而 \(a_2 = 4\) 是在 y 轴上的分量。

#### 1.3.2 向量的几何意义
- **起点**：通常在二维坐标系中，向量 \(\mathbf{a}\) 的起点是原点 \((0, 0)\)。
- **终点**：向量的终点是 \((3, 4)\)。因此，向量可以视为从原点到终点的箭头。

#### 1.3.3 向量的大小（模）
向量的大小（或模）是指从起点到终点的距离，可以用欧几里得距离公式计算。对于向量 \(\mathbf{a}\)，大小的计算公式如下：
\[
\|\mathbf{a}\| = \sqrt{a_1^2 + a_2^2}
\]
将具体的值代入公式：
\[
\|\mathbf{a}\| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5
\]
这意味着向量 \(\mathbf{a}\) 的大小为 5 个单位。

#### 1.3.4 向量的单位向量
单位向量是指大小为 1 的向量，它表示方向而不关心大小。单位向量可以通过将原始向量除以它的大小来获得：
\[
\hat{\mathbf{a}} = \frac{\mathbf{a}}{\|\mathbf{a}\|} = \begin{pmatrix} \frac{3}{5} \\ \frac{4}{5} \end{pmatrix}
\]
这表示向量 \(\hat{\mathbf{a}}\) 的大小为 1，其方向与向量 \(\mathbf{a}\) 相同。

#### 1.3.5 向量的方向
向量的方向可以通过计算与 x 轴的夹角来确定。夹角 \(\theta\) 可以通过三角函数求得：
\[
\tan(\theta) = \frac{a_2}{a_1} = \frac{4}{3}
\]
因此，\(\theta\) 的值可以通过反正切函数计算：
\[
\theta = \tan^{-1}\left(\frac{4}{3}\right)
\]
可以使用计算器或软件得到具体的角度值。


### 1.4 向量运算

向量运算是线性代数中的基本操作，主要包括向量加法和标量乘法。下面将详细解释这两个运算及其性质。

#### 1.4.1 向量加法
向量加法是将两个向量相加以得到一个新的向量。具体来说，对于两个二维向量 \(\mathbf{a}\) 和 \(\mathbf{b}\)，我们可以表示为：
\[
\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
\]
加法的公式为：
\[
\mathbf{a} + \mathbf{b} = \begin{pmatrix} a_1 + b_1 \\ a_2 + b_2 \end{pmatrix}
\]

**几何解释**：
- 向量加法可以通过平行四边形法则或三角形法则来可视化。
- 若将 \(\mathbf{a}\) 和 \(\mathbf{b}\) 以相同的起点绘制，则 \(\mathbf{a} + \mathbf{b}\) 的终点是从起点到 \(\mathbf{b}\) 终点，再从 \(\mathbf{b}\) 的起点到 \(\mathbf{a}\) 终点的直线。

**示例**：
设有向量：
\[
\mathbf{a} = \begin{pmatrix} 2 \\ 3 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 1 \\ 4 \end{pmatrix}
\]
则它们的和为：
\[
\mathbf{a} + \mathbf{b} = \begin{pmatrix} 2 + 1 \\ 3 + 4 \end{pmatrix} = \begin{pmatrix} 3 \\ 7 \end{pmatrix}
\]

#### 1.4.2 标量乘法
标量乘法是将一个向量的每个分量乘以一个标量（常数）。对于一个标量 \(c\) 和向量 \(\mathbf{a}\)，标量乘法的公式为：
\[
c \cdot \mathbf{a} = \begin{pmatrix} c \cdot a_1 \\ c \cdot a_2 \end{pmatrix}
\]

**几何解释**：
- 标量乘法可以看作是对向量的伸缩。
- 当 \(c > 1\) 时，向量的大小增大，方向不变；当 \(0 < c < 1\) 时，向量缩小；当 \(c < 0\) 时，向量的方向反向并可能缩小或扩大。

**示例**：
假设 \(c = 2\) 且 \(\mathbf{a} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}\)，则标量乘法的结果为：
\[
2 \cdot \mathbf{a} = \begin{pmatrix} 2 \cdot 3 \\ 2 \cdot 4 \end{pmatrix} = \begin{pmatrix} 6 \\ 8 \end{pmatrix}
\]

#### 1.4.3 向量内积
向量内积（也称点积）是两个向量的乘积，结果是一个标量。对于两个向量 \(\mathbf{a}\) 和 \(\mathbf{b}\)，其内积定义为：
\[
\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2
\]
其中 \(\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}\) 和 \(\mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}\)。

**几何解释**
向量内积也可以通过其几何意义来理解。对于任意两个向量 \(\mathbf{a}\) 和 \(\mathbf{b}\)，其内积可以用它们的模（长度）和夹角 \(\theta\) 表示：
\[
\mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos(\theta)
\]
其中 \(\|\mathbf{a}\|\) 和 \(\|\mathbf{b}\|\) 是向量的模，定义为：
\[
\|\mathbf{a}\| = \sqrt{a_1^2 + a_2^2}, \quad \|\mathbf{b}\| = \sqrt{b_1^2 + b_2^2}
\]

**向量内积的推导**

向量内积（点积）是两个向量相乘得到的一个标量。考虑两个二维向量：

\[
\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
\]

**代数定义**
向量 \(\mathbf{a}\) 和 \(\mathbf{b}\) 的内积定义为：
\[
\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2
\]
这一公式直接通过将相应分量相乘并相加得到。



**等式的推导**
结合代数定义与几何解释，我们可以得到：
\[
a_1 b_1 + a_2 b_2 = \|\mathbf{a}\| \|\mathbf{b}\| \cos(\theta)
\]
在特定情况下，当 \(\mathbf{a}\) 和 \(\mathbf{b}\) 处于相同的坐标系中，并且 \(\theta\) 是它们之间的夹角时，内积可以通过各分量的乘积加总得出。


**示例**：
设有向量：
\[
\mathbf{a} = \begin{pmatrix} 2 \\ 3 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 1 \\ 4 \end{pmatrix}
\]
则它们的内积为：
\[
\mathbf{a} \cdot \mathbf{b} = 2 \cdot 1 + 3 \cdot 4 = 2 + 12 = 14
\]

#### 1.4.4 向量外积
向量外积（也称叉积）是只适用于三维向量的运算，其结果是一个新的向量。对于两个三维向量 \(\mathbf{a}\) 和 \(\mathbf{b}\)，外积定义为：
\[
\mathbf{a} \times \mathbf{b} = \begin{pmatrix} a_2 b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{pmatrix}
\]
其中 \(\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}\) 和 \(\mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}\)。

**几何解释**：
- 向量外积的几何意义是得到一个垂直于 \(\mathbf{a}\) 和 \(\mathbf{b}\) 的向量，其模等于这两个向量所张成的平行四边形的面积：
\[
\|\mathbf{a} \times \mathbf{b}\| = \|\mathbf{a}\| \|\mathbf{b}\| \sin(\theta)
\]

**示例**：
设有向量：
\[
\mathbf{a} = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 4 \\ 5 \\ 6 \end{pmatrix}
\]
则它们的外积为：
\[
\mathbf{a} \times \mathbf{b} = \begin{pmatrix} 2 \cdot 6 - 3 \cdot 5 \\ 3 \cdot 4 - 1 \cdot 6 \\ 1 \cdot 5 - 2 \cdot 4 \end{pmatrix} = \begin{pmatrix} 12 - 15 \\ 12 - 6 \\ 5 - 8 \end{pmatrix} = \begin{pmatrix} -3 \\ 6 \\ -3 \end{pmatrix}
\]

#### 1.4.3 向量运算的性质
向量运算遵循一些重要的性质：

- **加法的交换律**：
  \[
  \mathbf{a} + \mathbf{b} = \mathbf{b} + \mathbf{a}
  \]

- **加法的结合律**：
  \[
  \mathbf{a} + (\mathbf{b} + \mathbf{c}) = (\mathbf{a} + \mathbf{b}) + \mathbf{c}
  \]

- **标量乘法的结合律**：
  \[
  c \cdot (d \cdot \mathbf{a}) = (c \cdot d) \cdot \mathbf{a}
  \]

- **标量乘法的分配律**：
  \[
  c \cdot (\mathbf{a} + \mathbf{b}) = c \cdot \mathbf{a} + c \cdot \mathbf{b}
  \]
  \[
  (c + d) \cdot \mathbf{a} = c \cdot \mathbf{a} + d \cdot \mathbf{a}
  \]


## 2. 向量空间

### 2.1 定义
向量空间是由向量组成的集合，这些向量可以进行加法和标量乘法。

### 2.2 线性组合
如果有向量\(\mathbf{v_1}, \mathbf{v_2}, \ldots, \mathbf{v_n}\)，那么它们的线性组合为：
\[
c_1 \mathbf{v_1} + c_2 \mathbf{v_2} + \ldots + c_n \mathbf{v_n}
\]
其中\(c_1, c_2, \ldots, c_n\)是标量。

## 3. 梯度向量

### 3.1 定义
梯度向量是一个多变量函数的偏导数向量，表示函数在某一点的最陡上升方向。
 \( f: \mathbb{R}^n \to \mathbb{R} \) 是一个多变量实值函数，梯度向量定义为：

\[
\nabla f(\mathbf{x}) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)
\]

这里，\(\nabla f(\mathbf{x})\) 是在点 \(\mathbf{x} = (x_1, x_2, \ldots, x_n)\) 的梯度，包含了所有偏导数。

梯度向量不仅仅表示变化的方向，它的方向本身就是最陡峭上升的方向。这是因为：（想象一个一维度的切线方程为示例）

- **单变量函数的直观类比**：在单变量函数 \( f(x) \) 中，函数的斜率 \( f'(x) \) 在某一点 \( x_0 \) 指示了函数在该点的上升方向。如果 \( f'(x_0) > 0 \)，则函数在 \( x_0 \) 处上升；如果 \( f'(x_0) < 0 \)，则下降。而当斜率为零时，则是一个极值点。

- **多变量扩展**：在多变量情形中，梯度 \(\nabla f(\mathbf{x})\) 可以视作所有偏导数的集合。根据线性代数的原理，梯度向量的方向是使得函数 \( f \) 变化最快的方向。


### 3.2 表示
假设有一个标量函数 \(f(x, y)\)，其梯度向量表示为：
\[
\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{pmatrix}
\]

### 3.3 计算
对于函数 \(f(x, y) = x^2 + y^2\)，梯度计算如下：
\[
\nabla f = \begin{pmatrix} \frac{\partial}{\partial x}(x^2 + y^2) \\ \frac{\partial}{\partial y}(x^2 + y^2) \end{pmatrix} = \begin{pmatrix} 2x \\ 2y \end{pmatrix}
\]

### 3.4 几何意义
- 梯度的方向指向函数值增加最快的方向。
- 梯度的大小表示在该方向上函数值的变化速率。

## 4. 梯度的应用

### 4.1 最优化问题
在最优化问题中，梯度用于找到函数的极小值或极大值。使用梯度下降法，通过不断更新位置来最小化目标函数：
\[
\mathbf{x}_{\text{new}} = \mathbf{x}_{\text{old}} - \eta \nabla f(\mathbf{x}_{\text{old}})
\]
其中\(\eta\)是学习率。

例如：求$f(x)=(x-3)^2$的最小数值
```python
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei'] # 步骤一（替换sans-serif字体）

# 定义目标函数 f(x) 和其梯度 g(x)
def f(x):
    return (x - 3) ** 2

def gradient(x):
    return 2 * (x - 3)

# 梯度下降法
def gradient_descent(starting_point, learning_rate, num_iterations):
    x_old = starting_point
    x_history = [x_old]  # 记录每一步的 x 值

    for _ in range(num_iterations):
        x_new = x_old - learning_rate * gradient(x_old)  # 更新位置
        x_history.append(x_new)
        x_old = x_new

    return x_new, x_history


# 参数设置

starting_point = 10.0  # 起始点
learning_rate = 0.1  # 学习率
num_iterations = 100  # 迭代次数

# 执行梯度下降法
final_x, history = gradient_descent(starting_point, learning_rate, num_iterations)

# 输出结果
print(f"最小值点: {final_x}")
print(f"目标函数在最小值点的值: {f(final_x)}")

# 绘制结果
x = np.linspace(-1, starting_point+1, 100)
plt.plot(x, f(x), label='f(x) = (x - 3)^2')
plt.scatter(history, f(np.array(history)), color='red', label='迭代路径')
plt.title('梯度下降法优化过程')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.axhline(0, color='gray', lw=0.5, ls='--')
plt.axvline(3, color='green', lw=0.5, ls='--', label='最小值位置 x = 3')
plt.legend()
plt.grid()
plt.show()

```

